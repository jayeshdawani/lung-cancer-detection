{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named cv2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e44beed91003>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdicom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named cv2"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "__author__ = 'ZFTurbo: https://kaggle.com/zfturbo'\n",
    "\n",
    "# This is simple script with many limitation due to run on Kaggle CPU server.\n",
    "# There is used simple CNN with low number of conv layers and filters.\n",
    "# You can improve this script while run on local GPU just by changing some constants\n",
    "# It just shows the possible example of dataflow which can be used for solving this problem\n",
    "\n",
    "conf = dict()\n",
    "# Change this variable to 0 in case you want to use full dataset\n",
    "conf['use_sample_only'] = 1\n",
    "# Save weights\n",
    "conf['save_weights'] = 0\n",
    "# How many patients will be in train and validation set during training. Range: (0; 1)\n",
    "conf['train_valid_fraction'] = 0.5\n",
    "# Batch size for CNN [Depends on GPU and memory available]\n",
    "conf['batch_size'] = 200\n",
    "# Number of epochs for CNN training\n",
    "conf['nb_epoch'] = 1\n",
    "# Early stopping. Stop training after epochs without improving on validation\n",
    "conf['patience'] = 3\n",
    "# Shape of image for CNN (Larger the better, but you need to increase CNN as well)\n",
    "conf['image_shape'] = (64, 64)\n",
    "# Learning rate for CNN. Lower better accuracy, larger runtime.\n",
    "conf['learning_rate'] = 1e-2\n",
    "# Number of random samples to use during training per epoch \n",
    "conf['samples_train_per_epoch'] = 10000\n",
    "# Number of random samples to use during validation per epoch\n",
    "conf['samples_valid_per_epoch'] = 1000\n",
    "# Some variables to control CNN structure\n",
    "conf['level_1_filters'] = 4\n",
    "conf['level_2_filters'] = 8\n",
    "conf['dense_layer_size'] = 32\n",
    "conf['dropout_value'] = 0.5\n",
    "\n",
    "\n",
    "import dicom \n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "np.random.seed(2016)\n",
    "random.seed(2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_and_normalize_dicom(path, x, y):\n",
    "    dicom1 = dicom.read_file(path)\n",
    "    dicom_img = dicom1.pixel_array.astype(np.float64)\n",
    "    mn = dicom_img.min()\n",
    "    mx = dicom_img.max()\n",
    "    if (mx - mn) != 0:\n",
    "        dicom_img = (dicom_img - mn)/(mx - mn)\n",
    "    else:\n",
    "        dicom_img[:, :] = 0\n",
    "    if dicom_img.shape != (x, y):\n",
    "        dicom_img = cv2.resize(dicom_img, (x, y), interpolation=cv2.INTER_CUBIC)\n",
    "    return dicom_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_generator_train(files, train_csv_table, batch_size):\n",
    "    number_of_batches = np.ceil(len(files)/batch_size)\n",
    "    counter = 0\n",
    "    random.shuffle(files)\n",
    "    while True:\n",
    "        batch_files = files[batch_size*counter:batch_size*(counter+1)]\n",
    "        image_list = []\n",
    "        mask_list = []\n",
    "        for f in batch_files:\n",
    "            image = load_and_normalize_dicom(f, conf['image_shape'][0], conf['image_shape'][1])\n",
    "            patient_id = os.path.basename(os.path.dirname(f))\n",
    "            is_cancer = train_csv_table.loc[train_csv_table['id'] == patient_id]['cancer'].values[0]\n",
    "            if is_cancer == 0:\n",
    "                mask = [1, 0]\n",
    "            else:\n",
    "                mask = [0, 1]\n",
    "            image_list.append([image])\n",
    "            mask_list.append(mask)\n",
    "        counter += 1\n",
    "        image_list = np.array(image_list)\n",
    "        mask_list = np.array(mask_list)\n",
    "        # print(image_list.shape)\n",
    "        # print(mask_list.shape)\n",
    "        yield image_list, mask_list\n",
    "        if counter == number_of_batches:\n",
    "            random.shuffle(files)\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_custom_CNN():\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1, 1), input_shape=(1, conf['image_shape'][0], conf['image_shape'][1]), dim_ordering='th'))\n",
    "    model.add(Convolution2D(conf['level_1_filters'], 3, 3, activation='relu', dim_ordering='th'))\n",
    "    model.add(ZeroPadding2D((1, 1), dim_ordering='th'))\n",
    "    model.add(Convolution2D(conf['level_1_filters'], 3, 3, activation='relu', dim_ordering='th'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), dim_ordering='th'))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1), dim_ordering='th'))\n",
    "    model.add(Convolution2D(conf['level_2_filters'], 3, 3, activation='relu', dim_ordering='th'))\n",
    "    model.add(ZeroPadding2D((1, 1), dim_ordering='th'))\n",
    "    model.add(Convolution2D(conf['level_2_filters'], 3, 3, activation='relu', dim_ordering='th'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), dim_ordering='th'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(conf['dense_layer_size'], activation='relu'))\n",
    "    model.add(Dropout(conf['dropout_value']))\n",
    "    model.add(Dense(conf['dense_layer_size'], activation='relu'))\n",
    "    model.add(Dropout(conf['dropout_value']))\n",
    "\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    sgd = SGD(lr=conf['learning_rate'], decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_train_single_fold(train_data, fraction):\n",
    "    ids = train_data['id'].values\n",
    "    random.shuffle(ids)\n",
    "    split_point = int(round(fraction*len(ids)))\n",
    "    train_list = ids[:split_point]\n",
    "    valid_list = ids[split_point:]\n",
    "    return train_list, valid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_single_model():\n",
    "    train_csv_table = pd.read_csv('../input/stage1_labels.csv')\n",
    "    train_patients, valid_patients = get_train_single_fold(train_csv_table, conf['train_valid_fraction'])\n",
    "    print('Train patients: {}'.format(len(train_patients)))\n",
    "    print('Valid patients: {}'.format(len(valid_patients)))\n",
    "\n",
    "    print('Create and compile model...')\n",
    "    model = get_custom_CNN()\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=conf['patience'], verbose=0),\n",
    "        # ModelCheckpoint('best.hdf5', monitor='val_loss', save_best_only=True, verbose=0),\n",
    "    ]\n",
    "\n",
    "    get_dir = 'stage1'\n",
    "    if conf['use_sample_only'] == 1:\n",
    "        get_dir = 'sample_images'\n",
    "\n",
    "    train_files = []\n",
    "    for p in train_patients:\n",
    "        train_files += glob.glob(\"../input/{}/{}/*.dcm\".format(get_dir, p))\n",
    "    print('Number of train files: {}'.format(len(train_files)))\n",
    "\n",
    "    valid_files = []\n",
    "    for p in valid_patients:\n",
    "        valid_files += glob.glob(\"../input/{}/{}/*.dcm\".format(get_dir, p))\n",
    "    print('Number of valid files: {}'.format(len(valid_files)))\n",
    "\n",
    "    print('Fit model...')\n",
    "    # It's training on a list of all slices from all patients\n",
    "    # Some slices that don't have nodules will still be labeled as cancerous,\n",
    "    # if other slices from that patient have them\n",
    "    # Isn't this a source of error?\n",
    "    print('Samples train: {}, Samples valid: {}'.format(conf['samples_train_per_epoch'], conf['samples_valid_per_epoch']))\n",
    "    fit = model.fit_generator(\n",
    "        generator=batch_generator_train(train_files, train_csv_table, conf['batch_size']),\n",
    "        nb_epoch=conf['nb_epoch'],\n",
    "        samples_per_epoch=conf['samples_train_per_epoch'],\n",
    "        validation_data=batch_generator_train(valid_files, train_csv_table, conf['batch_size']),\n",
    "        nb_val_samples=conf['samples_valid_per_epoch'],\n",
    "        verbose=1,\n",
    "        callbacks=callbacks)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_submission(model):\n",
    "    sample_subm = pd.read_csv(\"../input/stage1_sample_submission.csv\")\n",
    "    ids = sample_subm['id'].values\n",
    "    for id in ids:\n",
    "        print('Predict for patient {}'.format(id))\n",
    "        files = glob.glob(\"../input/stage1/{}/*.dcm\".format(id))\n",
    "        image_list = []\n",
    "        for f in files:\n",
    "            image = load_and_normalize_dicom(f, conf['image_shape'][0], conf['image_shape'][1])\n",
    "            image_list.append([image])\n",
    "        image_list = np.array(image_list)\n",
    "        batch_size = len(image_list)\n",
    "        predictions = model.predict(image, verbose=1, batch_size=batch_size)\n",
    "        pred_value = predictions[:, 1].mean()\n",
    "        sample_subm.loc[sample_subm['id'] == id, 'cancer'] = pred_value\n",
    "    sample_subm.to_csv(\"subm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do it\n",
    "model = create_single_model()\n",
    "if conf['save_weights'] == 1:\n",
    "    model.save_weights('mdl.h5')\n",
    "create_submission(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfkernel",
   "language": "python",
   "name": "tfkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
